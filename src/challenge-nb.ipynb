{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### INSTRUCTIONS ####\n",
    "\n",
    "# 1) Update EC_DIR (below) to reflect the location of the challenge files in your Google Drive as required.\n",
    "\n",
    "    # For example:\n",
    "\n",
    "        # 1) Navigate to Google Drive: https://drive.google.com/drive/my-drive and log in\n",
    "        # 2) Create a folder called 'ecc_files' in the root directory (should be called 'MyDrive')\n",
    "        # 3) Upload the files to the folder\n",
    "        \n",
    "# 2) Connect to runtime (there are CPUs that can be used for free if you don't have any compute units available for GPU use but these might take a while to run)\n",
    "\n",
    "# 3) Run the rest of the notebook (you will be prompted to grant access to your Google Drive)\n",
    "\n",
    "EC_DIR = \"/content/drive/MyDrive/ecc_files\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "#  Setup: Install Dependencies and Mount Drive (Colab)\n",
    "############################################################\n",
    "\n",
    "# Using PyTorch geometric implementation of Node2Vec to leverage GPU support\n",
    "# using pre-built wheels to speed up the process\n",
    "\n",
    "import os # noqa: F401\n",
    "import torch # noqa: F401\n",
    "from google.colab import drive\n",
    "\n",
    "# Install PyTorch Geometric dependencies compatible with the current PyTorch version\n",
    "def install_pytorch_geometric():\n",
    "    TORCH_VERSION = torch.__version__.split(\"+\")[0]  # noqa: F841\n",
    "    CUDA_VERSION = torch.version.cuda.replace(\".\", \"\")  # noqa: F841\n",
    "    base_url = \"https://data.pyg.org/whl\"  # noqa: F841\n",
    "    \n",
    "    # Install each dependency from the PyTorch Geometric library\n",
    "    !pip install torch-scatter -f {base_url}/torch-{TORCH_VERSION}+cu{CUDA_VERSION}.html\n",
    "    !pip install torch-sparse -f {base_url}/torch-{TORCH_VERSION}+cu{CUDA_VERSION}.html\n",
    "    !pip install torch-cluster -f {base_url}/torch-{TORCH_VERSION}+cu{CUDA_VERSION}.html\n",
    "    !pip install torch-spline-conv -f {base_url}/torch-{TORCH_VERSION}+cu{CUDA_VERSION}.html\n",
    "    !pip install pyg-lib -f {base_url}/torch-{TORCH_VERSION}+cu{CUDA_VERSION}.html\n",
    "    !pip install torch-geometric\n",
    "\n",
    "install_pytorch_geometric()\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount(\"/content/drive\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "#  Import remaining libraries\n",
    "############################################################\n",
    "\n",
    "import pickle\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Tuple, List, Any, Optional\n",
    "\n",
    "import networkx as nx\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from torch import nn\n",
    "from torch.cuda.amp import autocast\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "from torch_geometric.nn import Node2Vec\n",
    "from torch_geometric.data import Data as PyGData\n",
    "from torch_geometric.utils import from_networkx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "# 1) Device Settings\n",
    "############################################################\n",
    "# Use GPU if available\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "############################################################\n",
    "# 2) Model & Embedding Dimensions\n",
    "############################################################\n",
    "# Name of the pre-trained model to use for semantic embeddings\n",
    "# MODEL_NAME_SEMANTIC = \"dmis-lab/biobert-v1.1\"\n",
    "# MODEL_NAME_SEMANTIC = \"michiyasunaga/BioLinkBERT-base\"\n",
    "# MODEL_NAME_SEMANTIC = \"sultan/BioM-BERT-PubMed-PMC-Large\"\n",
    "MODEL_NAME_SEMANTIC = \"bioformers/bioformer-16L\"\n",
    "\n",
    "# Extracting model name to avoid path construction issues\n",
    "MODEL_NAME = MODEL_NAME_SEMANTIC.split(\"/\")[-1]\n",
    "\n",
    "# Dimensionality of the semantic embeddings\n",
    "EMBED_DIM_SEMANTIC = 768\n",
    "\n",
    "# Dimensionality of the structural embeddings\n",
    "EMBED_DIM_STRUCT = 128\n",
    "\n",
    "# Combined dimensionality for the hybrid embeddings (semantic + structural)\n",
    "EMBED_DIM_HYBRID = EMBED_DIM_SEMANTIC + EMBED_DIM_STRUCT\n",
    "\n",
    "############################################################\n",
    "# 3) Data & I/O Directories\n",
    "############################################################\n",
    "\n",
    "# Paths to the CSV files containing nodes, edges and ground truth information\n",
    "NODES_CSV = os.path.join(EC_DIR, \"Nodes.csv\")\n",
    "EDGES_CSV = os.path.join(EC_DIR, \"Edges.csv\")\n",
    "GROUND_TRUTH_CSV = os.path.join(EC_DIR, \"Ground Truth.csv\")\n",
    "\n",
    "# File path to store or load structural embeddings\n",
    "STRUCT_EMB_PATH = os.path.join(EC_DIR, \"structural_embeddings.pkl\")\n",
    "\n",
    "# File path to store or load semantic embeddings\n",
    "SEMANTIC_EMB_PATH = os.path.join(EC_DIR, f\"{MODEL_NAME}_semantic_embeddings.pkl\")\n",
    "\n",
    "# File path to store or load hybrid embeddings (semantic + structural)\n",
    "HYBRID_EMB_PATH = os.path.join(EC_DIR, f\"{MODEL_NAME}_hybrid_embeddings.pkl\")\n",
    "\n",
    "############################################################\n",
    "# 4) Node2Vec Hyperparameters\n",
    "############################################################\n",
    "\n",
    "# NODE2VEC_EMB_DIM:\n",
    "#   The dimensionality of the learned node embeddings. Larger dimensions can\n",
    "#   capture more nuanced relationships but require more memory and can risk overfitting.\n",
    "NODE2VEC_EMB_DIM = EMBED_DIM_STRUCT\n",
    "\n",
    "# NODE2VEC_WALK_LENGTH:\n",
    "#   The number of steps in each random walk. A higher walk length lets the model\n",
    "#   capture more distant structural patterns but increases computational cost.\n",
    "NODE2VEC_WALK_LENGTH = 20\n",
    "\n",
    "# NODE2VEC_CONTEXT_SIZE:\n",
    "#   The “window size” for the skip-gram model—how many nodes to each side of a \n",
    "#   target node are considered part of its context. A larger context includes\n",
    "#   more neighborhood information but may dilute the most local signals.\n",
    "NODE2VEC_CONTEXT_SIZE = 10\n",
    "\n",
    "# NODE2VEC_WALKS_PER_NODE:\n",
    "#   How many random walks to start from each node. More walks can lead to richer \n",
    "#   co-occurrence statistics for skip-gram training, but also increase runtime.\n",
    "NODE2VEC_WALKS_PER_NODE = 20\n",
    "\n",
    "# NODE2VEC_EPOCHS:\n",
    "#   The number of epochs (full passes) over all generated random walks during \n",
    "#   training. Too few may underfit; too many risks overfitting or diminishing returns.\n",
    "NODE2VEC_EPOCHS = 10\n",
    "\n",
    "# NODE2VEC_LR:\n",
    "#   The learning rate for optimising the Node2Vec model (here using SparseAdam).\n",
    "#   Higher values converge faster but can be unstable; lower values are more stable\n",
    "#   but slower to converge.\n",
    "NODE2VEC_LR = 0.01\n",
    "\n",
    "# NODE2VEC_BATCH_SIZE:\n",
    "#   The batch size for training on random walk “samples” in skip-gram.\n",
    "#   Larger batches can be faster on GPUs but need more memory; smaller batches\n",
    "#   can sometimes generalise better but may take longer to train.\n",
    "NODE2VEC_BATCH_SIZE = 128\n",
    "\n",
    "\n",
    "############################################################\n",
    "# 5) Semantic Embedding Hyperparameters\n",
    "############################################################\n",
    "# Batch size for processing texts when generating semantic embeddings\n",
    "SEMANTIC_BATCH_SIZE = 128\n",
    "\n",
    "# Maximum sequence length when tokenising texts for semantic embeddings\n",
    "SEMANTIC_MAX_LENGTH = 128\n",
    "\n",
    "############################################################\n",
    "# 6) Classifier Hyperparameters\n",
    "############################################################\n",
    "# Neural network (MLP) training: default number of epochs\n",
    "TRAIN_CLASSIFIER_EPOCHS = 5\n",
    "\n",
    "# Neural network (MLP) training: default learning rate\n",
    "TRAIN_CLASSIFIER_LR = 1e-3\n",
    "\n",
    "# Neural network (MLP) training: default batch size\n",
    "TRAIN_CLASSIFIER_BATCH_SIZE = 256\n",
    "\n",
    "# Logistic Regression: maximum iterations for solver convergence\n",
    "LOGREG_MAX_ITER = 1000\n",
    "\n",
    "############################################################\n",
    "# 7) Splits & Seeds\n",
    "############################################################\n",
    "# Proportion of dataset to be used as the test set\n",
    "TEST_SPLIT = 0.20\n",
    "\n",
    "# Proportion of (train) dataset to be used for validation\n",
    "VAL_SPLIT = 0.20\n",
    "\n",
    "# Seed for random number generators (reproducibility)\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "############################################################\n",
    "# 8) MLP Hyperparameter Search\n",
    "############################################################\n",
    "# MLP hyperparameter search: possible hidden dimensions\n",
    "MLP_HIDDEN_DIMS = [128, 256]\n",
    "\n",
    "# MLP hyperparameter search: possible learning rates\n",
    "MLP_LRS = [1e-3, 5e-4]\n",
    "\n",
    "# MLP hyperparameter search: possible epochs\n",
    "MLP_EPOCH_CHOICES = [5, 10]\n",
    "\n",
    "print(\"=== Parameter Configuration ===\")\n",
    "print(f\"DEVICE: {DEVICE}\")\n",
    "print(f\"MODEL_NAME_SEMANTIC: {MODEL_NAME_SEMANTIC}\")\n",
    "print(f\"EMBED_DIM_SEMANTIC: {EMBED_DIM_SEMANTIC}\")\n",
    "print(f\"EMBED_DIM_STRUCT: {EMBED_DIM_STRUCT}\")\n",
    "print(f\"EMBED_DIM_HYBRID: {EMBED_DIM_HYBRID}\")\n",
    "print(\"================================\\n\")\n",
    "\n",
    "\n",
    "############################################################\n",
    "# Step 1: Data Loading & Preprocessing\n",
    "############################################################\n",
    "def load_data(nodes_csv: str, edges_csv: str, gt_csv: str) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Loads data from CSV files.\n",
    "\n",
    "    :param nodes_csv: File path to the nodes CSV.\n",
    "    :param edges_csv: File path to the edges CSV.\n",
    "    :param gt_csv: File path to the ground truth CSV.\n",
    "    :return: A tuple containing the nodes, edges and ground truth dataframes.\n",
    "    \"\"\"\n",
    "    nodes_df = pd.read_csv(nodes_csv)\n",
    "    edges_df = pd.read_csv(edges_csv)\n",
    "    gt_df = pd.read_csv(gt_csv)\n",
    "    return nodes_df, edges_df, gt_df\n",
    "\n",
    "\n",
    "def build_node_index(nodes_df: pd.DataFrame) -> Tuple[dict, List[Any], List[str]]:\n",
    "    \"\"\"\n",
    "    Builds node index mappings and extracts textual data for each node.\n",
    "\n",
    "    :param nodes_df: DataFrame containing node information.\n",
    "    :return: A tuple of (node-to-index dictionary, index-to-node list, list of node texts).\n",
    "    \"\"\"\n",
    "    unique_ids = nodes_df[\"id\"].tolist()\n",
    "    node2idx = {nid: i for i, nid in enumerate(unique_ids)}\n",
    "    idx2node = [nid for nid in unique_ids]\n",
    "\n",
    "    node_texts = []\n",
    "    for _, row in nodes_df.iterrows():\n",
    "        name = str(row.get(\"name\", \"\"))\n",
    "        desc = str(row.get(\"description\", \"\"))\n",
    "        text = (name + \" \" + desc).strip()\n",
    "        if not text:\n",
    "            text = \"No description\"\n",
    "        node_texts.append(text)\n",
    "\n",
    "    return node2idx, idx2node, node_texts\n",
    "\n",
    "\n",
    "############################################################\n",
    "# Step 2: Structural Embeddings via PyTorch Geometric Node2Vec\n",
    "############################################################\n",
    "def build_graph_pyg(edges_df: pd.DataFrame, node2idx: dict) -> PyGData:\n",
    "    \"\"\"\n",
    "    Builds a PyTorch Geometric graph from the edges dataframe and node index.\n",
    "\n",
    "    :param edges_df: DataFrame containing edges information with columns 'subject' and 'object'.\n",
    "    :param node2idx: A dictionary mapping node IDs to indices.\n",
    "    :return: A PyGData graph object.\n",
    "    \"\"\"\n",
    "    G = nx.Graph()\n",
    "    for n in node2idx.values():\n",
    "        G.add_node(n)\n",
    "\n",
    "    for _, row in edges_df.iterrows():\n",
    "        s, o = row[\"subject\"], row[\"object\"]\n",
    "        if s in node2idx and o in node2idx:\n",
    "            G.add_edge(node2idx[s], node2idx[o])\n",
    "\n",
    "    pyg_data = from_networkx(G)\n",
    "    return pyg_data\n",
    "\n",
    "\n",
    "def generate_node2vec_embeddings_pyg(\n",
    "    data: PyGData,\n",
    "    embedding_dim: int = NODE2VEC_EMB_DIM,\n",
    "    walk_length: int = NODE2VEC_WALK_LENGTH,\n",
    "    context_size: int = NODE2VEC_CONTEXT_SIZE,\n",
    "    walks_per_node: int = NODE2VEC_WALKS_PER_NODE,\n",
    "    epochs: int = NODE2VEC_EPOCHS,\n",
    "    lr: float = NODE2VEC_LR,\n",
    "    batch_size: int = NODE2VEC_BATCH_SIZE,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generates structural embeddings using PyTorch Geometric's Node2Vec.\n",
    "\n",
    "    :param data: PyGData graph.\n",
    "    :param embedding_dim: Dimensionality of the embeddings.\n",
    "    :param walk_length: Length of each random walk.\n",
    "    :param context_size: Context size for Skip-Gram.\n",
    "    :param walks_per_node: Number of random walks per node.\n",
    "    :param epochs: Number of training epochs.\n",
    "    :param lr: Learning rate for the optimiser.\n",
    "    :param batch_size: Batch size for training.\n",
    "    :return: Numpy array of node embeddings.\n",
    "    \"\"\"\n",
    "    print(\"Initialising PyTorch Geometric Node2Vec...\")\n",
    "    node2vec = Node2Vec(\n",
    "        edge_index=data.edge_index,\n",
    "        embedding_dim=embedding_dim,\n",
    "        walk_length=walk_length,\n",
    "        context_size=context_size,\n",
    "        walks_per_node=walks_per_node,\n",
    "        num_negative_samples=1,\n",
    "        sparse=True,\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    optimiser = torch.optim.SparseAdam(node2vec.parameters(), lr=lr)\n",
    "\n",
    "    print(\"Training Node2Vec embeddings...\")\n",
    "    node2vec.train()\n",
    "    loader = node2vec.loader(batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for pos_rw, neg_rw in loader:\n",
    "            pos_rw, neg_rw = pos_rw.to(DEVICE), neg_rw.to(DEVICE)\n",
    "            optimiser.zero_grad()\n",
    "            loss = node2vec.loss(pos_rw, neg_rw)\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "    print(\"Extracting embeddings...\")\n",
    "    node2vec.eval()\n",
    "    embeddings = node2vec.embedding.weight.cpu().detach().numpy()\n",
    "    print(\"Embeddings generated successfully.\")\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "############################################################\n",
    "# Step 3: Semantic Embeddings\n",
    "############################################################\n",
    "class SemanticEmbedder:\n",
    "    \"\"\"\n",
    "    Class for generating semantic embeddings using a pre-trained transformer model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str) -> None:\n",
    "        \"\"\"\n",
    "        Initialise the semantic embedder.\n",
    "\n",
    "        :param model_name: Pre-trained model name.\n",
    "        \"\"\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "        self.model = AutoModel.from_pretrained(model_name).to(DEVICE)\n",
    "        self.model.eval()\n",
    "\n",
    "    def encode_texts(\n",
    "        self,\n",
    "        texts: List[str],\n",
    "        batch_size: int = SEMANTIC_BATCH_SIZE,\n",
    "        max_length: int = SEMANTIC_MAX_LENGTH,\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Encodes a list of texts into semantic embeddings.\n",
    "\n",
    "        :param texts: List of textual descriptions.\n",
    "        :param batch_size: Batch size for processing.\n",
    "        :param max_length: Maximum sequence length for tokenisation.\n",
    "        :return: Numpy array of embeddings.\n",
    "        \"\"\"\n",
    "        all_embeddings = []\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i : i + batch_size]\n",
    "            enc = self.tokenizer(\n",
    "                batch,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=max_length,\n",
    "                return_tensors=\"pt\",\n",
    "            ).to(DEVICE)\n",
    "            with torch.no_grad():\n",
    "                with autocast():\n",
    "                    outputs = self.model(**enc)\n",
    "                    cls_emb = outputs.last_hidden_state[:, 0, :]\n",
    "            all_embeddings.append(cls_emb.cpu().numpy())\n",
    "        return np.concatenate(all_embeddings, axis=0)\n",
    "\n",
    "\n",
    "############################################################\n",
    "# Step 4: Combine Structural & Semantic Embeddings\n",
    "############################################################\n",
    "def build_hybrid_embeddings(semantic_emb: np.ndarray, structural_emb: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Concatenates semantic and structural embeddings.\n",
    "\n",
    "    :param semantic_emb: Semantic embeddings as a numpy array.\n",
    "    :param structural_emb: Structural embeddings as a numpy array.\n",
    "    :return: Hybrid embeddings as a concatenated numpy array.\n",
    "    \"\"\"\n",
    "    return np.concatenate((semantic_emb, structural_emb), axis=1)\n",
    "\n",
    "\n",
    "############################################################\n",
    "# Step 5: Prepare Dataset for Link Classification\n",
    "############################################################\n",
    "def prepare_dataset(gt_df: pd.DataFrame, node2idx: dict, embeddings: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Prepares features and labels for the link classification task.\n",
    "\n",
    "    :param gt_df: Ground truth dataframe containing source, target and label 'y'.\n",
    "    :param node2idx: A dictionary mapping node IDs to indices.\n",
    "    :param embeddings: Hybrid embeddings as a numpy array.\n",
    "    :return: Tuple of features and labels.\n",
    "    \"\"\"\n",
    "    pairs = gt_df[[\"source\", \"target\"]].values\n",
    "    labels = gt_df[\"y\"].values.astype(float)\n",
    "\n",
    "    X = []\n",
    "    for src, tgt in pairs:\n",
    "        if src in node2idx and tgt in node2idx:\n",
    "            src_idx = node2idx[src]\n",
    "            tgt_idx = node2idx[tgt]\n",
    "            pair_emb = np.concatenate([embeddings[src_idx], embeddings[tgt_idx]])\n",
    "        else:\n",
    "            pair_emb = np.zeros((embeddings.shape[1] * 2,))\n",
    "        X.append(pair_emb)\n",
    "    X = np.array(X)\n",
    "    return X, labels\n",
    "\n",
    "\n",
    "############################################################\n",
    "# Step 6: Classification & Evaluation\n",
    "############################################################\n",
    "class LinkClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple MLP for link classification.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_dim: int, hidden_dim: int = 128) -> None:\n",
    "        \"\"\"\n",
    "        Initialise the MLP.\n",
    "\n",
    "        :param in_dim: Input dimensionality.\n",
    "        :param hidden_dim: Hidden layer dimensionality.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the classifier.\n",
    "\n",
    "        :param x: Input tensor.\n",
    "        :return: Output tensor.\n",
    "        \"\"\"\n",
    "        return self.mlp(x)\n",
    "\n",
    "\n",
    "def train_classifier(\n",
    "    model: nn.Module,\n",
    "    X_train: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    X_val: np.ndarray,\n",
    "    y_val: np.ndarray,\n",
    "    epochs: int = TRAIN_CLASSIFIER_EPOCHS,\n",
    "    lr: float = TRAIN_CLASSIFIER_LR,\n",
    "    batch_size: int = TRAIN_CLASSIFIER_BATCH_SIZE,\n",
    ") -> nn.Module:\n",
    "    \"\"\"\n",
    "    Trains the classifier and evaluates on a validation set.\n",
    "\n",
    "    :param model: The link classifier model.\n",
    "    :param X_train: Training features.\n",
    "    :param y_train: Training labels.\n",
    "    :param X_val: Validation features.\n",
    "    :param y_val: Validation labels.\n",
    "    :param epochs: Number of training epochs.\n",
    "    :param lr: Learning rate.\n",
    "    :param batch_size: Batch size.\n",
    "    :return: The trained model.\n",
    "    \"\"\"\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    X_train_t = torch.tensor(X_train, dtype=torch.float32).to(DEVICE)\n",
    "    y_train_t = torch.tensor(y_train, dtype=torch.float32).view(-1, 1).to(DEVICE)\n",
    "    X_val_t = torch.tensor(X_val, dtype=torch.float32).to(DEVICE)\n",
    "    y_val_t = torch.tensor(y_val, dtype=torch.float32).view(-1, 1).to(DEVICE) # noqa: F841\n",
    "\n",
    "    best_val_auc = 0.0\n",
    "    best_model_state: Optional[Any] = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        perm = torch.randperm(X_train_t.size(0))\n",
    "        total_loss = 0.0\n",
    "        for i in range(0, X_train_t.size(0), batch_size):\n",
    "            idx = perm[i : i + batch_size]\n",
    "            xb = X_train_t[idx]\n",
    "            yb = y_train_t[idx]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            out = model(xb)\n",
    "            loss = criterion(out, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_out = model(X_val_t)\n",
    "            val_prob = torch.sigmoid(val_out).cpu().numpy().flatten()\n",
    "            val_auc = roc_auc_score(y_val, val_prob)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {total_loss:.4f} | Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_auc > best_val_auc:\n",
    "            best_val_auc = val_auc\n",
    "            best_model_state = model.state_dict()\n",
    "\n",
    "    if best_model_state:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    return model\n",
    "\n",
    "\n",
    "def hyperparam_search_mlp(\n",
    "    X_train: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    X_val: np.ndarray,\n",
    "    y_val: np.ndarray,\n",
    "    device: str = DEVICE,\n",
    ") -> Tuple[LinkClassifier, Tuple[int, float, int], float]:\n",
    "    \"\"\"\n",
    "    Conducts a grid search over hidden_dim, lr and epochs for the MLP.\n",
    "\n",
    "    :param X_train: Training features.\n",
    "    :param y_train: Training labels.\n",
    "    :param X_val: Validation features.\n",
    "    :param y_val: Validation labels.\n",
    "    :param device: Device to use.\n",
    "    :return: A tuple (best_model, best_config, best_val_auc) where best_config is (hidden_dim, lr, epochs).\n",
    "    \"\"\"\n",
    "    best_auc = 0.0\n",
    "    best_config: Optional[Tuple[int, float, int]] = None\n",
    "    best_model: Optional[LinkClassifier] = None\n",
    "\n",
    "    for hd, lr_, ep in itertools.product(MLP_HIDDEN_DIMS, MLP_LRS, MLP_EPOCH_CHOICES):\n",
    "        print(f\"\\n[Hyperparam Search] Trying hidden_dim={hd}, lr={lr_}, epochs={ep}\")\n",
    "        model = LinkClassifier(in_dim=X_train.shape[1], hidden_dim=hd)\n",
    "        trained_model = train_classifier(\n",
    "            model,\n",
    "            X_train,\n",
    "            y_train,\n",
    "            X_val,\n",
    "            y_val,\n",
    "            epochs=ep,\n",
    "            lr=lr_,\n",
    "            batch_size=TRAIN_CLASSIFIER_BATCH_SIZE,\n",
    "        )\n",
    "        # Validation AUC after training\n",
    "        X_val_t = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "        trained_model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_out = trained_model(X_val_t)\n",
    "            val_prob = torch.sigmoid(val_out).cpu().numpy().flatten()\n",
    "        auc_val = roc_auc_score(y_val, val_prob)\n",
    "\n",
    "        if auc_val > best_auc:\n",
    "            best_auc = auc_val\n",
    "            best_config = (hd, lr_, ep)\n",
    "            # Clone the model state\n",
    "            best_model = LinkClassifier(in_dim=X_train.shape[1], hidden_dim=hd).to(device)\n",
    "            best_model.load_state_dict(trained_model.state_dict())\n",
    "\n",
    "    print(\n",
    "        f\"\\n[Hyperparam Search] Best config: hidden_dim={best_config[0]}, lr={best_config[1]}, \"\n",
    "        f\"epochs={best_config[2]} with val AUC={best_auc:.4f}\\n\"\n",
    "    )\n",
    "    return best_model, best_config, best_auc\n",
    "\n",
    "# =========================================================\n",
    "#  NEW Helper: remove edges from edges_df for Node2Vec\n",
    "# =========================================================\n",
    "def remove_positive_edges_from_graph(\n",
    "    edges_df: pd.DataFrame,\n",
    "    gt_subset: pd.DataFrame,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Removes edges in 'gt_subset' (where y=1) from the main edges_df.\n",
    "    This ensures Node2Vec does not see future/val edges.\n",
    "    \n",
    "    :param edges_df: The full set of known edges (positive) in your graph.\n",
    "    :param gt_subset: The ground-truth edges you want removed (subset can be val or test).\n",
    "    :return: A filtered edges_df with the edges in gt_subset removed.\n",
    "    \"\"\"\n",
    "    # Keep only edges with label=1 in the subset\n",
    "    pos_edges = gt_subset[gt_subset[\"y\"] == 1][[\"source\", \"target\"]].values\n",
    "    \n",
    "    # Make a set for quick look-up\n",
    "    pos_edge_set = set()\n",
    "    for s, t in pos_edges:\n",
    "        # Because your graph is undirected in Node2Vec\n",
    "        pos_edge_set.add((s, t))\n",
    "        pos_edge_set.add((t, s))\n",
    "\n",
    "    mask = []\n",
    "    for row in edges_df.itertuples():\n",
    "        e = (row.subject, row.object)\n",
    "        mask.append(e not in pos_edge_set)\n",
    "    return edges_df[mask]\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "#  Modified main with no structural leakage\n",
    "# =========================================================\n",
    "def main():\n",
    "    print(\"Loading data...\")\n",
    "    nodes_df, edges_df, gt_df = load_data(NODES_CSV, EDGES_CSV, GROUND_TRUTH_CSV)\n",
    "\n",
    "    print(\"Building node indices and text data...\")\n",
    "    node2idx, idx2node, node_texts = build_node_index(nodes_df)\n",
    "\n",
    "    # -----------------------------------------------------\n",
    "    # 1) SPLIT ground truth edges (gt_df) into train/val/test\n",
    "    #    BEFORE building Node2Vec\n",
    "    # -----------------------------------------------------\n",
    "    gt_train, gt_test = train_test_split(\n",
    "        gt_df, test_size=TEST_SPLIT, random_state=RANDOM_SEED\n",
    "    )\n",
    "    gt_train, gt_val = train_test_split(\n",
    "        gt_train, test_size=VAL_SPLIT, random_state=RANDOM_SEED\n",
    "    )\n",
    "\n",
    "    # Optionally print distribution\n",
    "    print(\"\\nGround Truth Splits (pos rate might vary):\")\n",
    "    print(f\"  gt_train: {len(gt_train)} edges, ~{gt_train['y'].mean()*100:.1f}% positive\")\n",
    "    print(f\"  gt_val:   {len(gt_val)} edges, ~{gt_val['y'].mean()*100:.1f}% positive\")\n",
    "    print(f\"  gt_test:  {len(gt_test)} edges, ~{gt_test['y'].mean()*100:.1f}% positive\")\n",
    "\n",
    "    # -----------------------------------------------------\n",
    "    # 2) REMOVE val/test positive edges from edges_df\n",
    "    #    so Node2Vec doesn't see them\n",
    "    # -----------------------------------------------------\n",
    "    print(\"\\nRemoving val/test edges from the Node2Vec graph...\")\n",
    "    edges_filtered = remove_positive_edges_from_graph(edges_df, gt_val)\n",
    "    edges_filtered = remove_positive_edges_from_graph(edges_filtered, gt_test)\n",
    "    print(f\"Original edges: {len(edges_df)} | Filtered edges: {len(edges_filtered)}\")\n",
    "\n",
    "    # -----------------------------------------------------\n",
    "    # 3) Build Node2Vec graph (train only) & generate embeddings\n",
    "    # -----------------------------------------------------\n",
    "    if os.path.exists(STRUCT_EMB_PATH):\n",
    "        print(f\"Found existing {STRUCT_EMB_PATH}, loading...\")\n",
    "        with open(STRUCT_EMB_PATH, \"rb\") as f:\n",
    "            structural_embeddings = pickle.load(f)\n",
    "    else:\n",
    "        print(\"Building train_graph and generating Node2Vec embeddings...\")\n",
    "        pyg_graph = build_graph_pyg(edges_filtered, node2idx)\n",
    "        structural_embeddings = generate_node2vec_embeddings_pyg(pyg_graph)\n",
    "        with open(STRUCT_EMB_PATH, \"wb\") as f:\n",
    "            pickle.dump(structural_embeddings, f)\n",
    "\n",
    "    # -----------------------------------------------------\n",
    "    # 4) Semantic Embeddings (no leakage issue here)\n",
    "    # -----------------------------------------------------\n",
    "    if os.path.exists(SEMANTIC_EMB_PATH):\n",
    "        print(f\"Found existing {SEMANTIC_EMB_PATH}, loading...\")\n",
    "        with open(SEMANTIC_EMB_PATH, \"rb\") as f:\n",
    "            semantic_embeddings = pickle.load(f)\n",
    "    else:\n",
    "        print(\"No semantic embedding pickle found. Generating embeddings...\")\n",
    "        embedder = SemanticEmbedder(model_name=MODEL_NAME_SEMANTIC)\n",
    "        semantic_embeddings = embedder.encode_texts(node_texts)\n",
    "        with open(SEMANTIC_EMB_PATH, \"wb\") as f:\n",
    "            pickle.dump(semantic_embeddings, f)\n",
    "\n",
    "    # -----------------------------------------------------\n",
    "    # 5) Build final \"hybrid\" embeddings (semantic+structural)\n",
    "    # -----------------------------------------------------\n",
    "    hybrid_embeddings = build_hybrid_embeddings(semantic_embeddings, structural_embeddings)\n",
    "    with open(HYBRID_EMB_PATH, \"wb\") as f:\n",
    "        pickle.dump(hybrid_embeddings, f)\n",
    "\n",
    "    # -----------------------------------------------------\n",
    "    # 6) Prepare train/val/test sets using the split gt_dfs\n",
    "    #    (Now that embeddings are done)\n",
    "    # -----------------------------------------------------\n",
    "    print(\"\\nPreparing train/val/test feature tensors...\")\n",
    "    # Train\n",
    "    X_train, y_train = prepare_dataset(gt_train, node2idx, hybrid_embeddings)\n",
    "    # Val\n",
    "    X_val, y_val = prepare_dataset(gt_val, node2idx, hybrid_embeddings)\n",
    "    # Test\n",
    "    X_test, y_test = prepare_dataset(gt_test, node2idx, hybrid_embeddings)\n",
    "\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    print(f\"Train: {X_train.shape}   Val: {X_val.shape}   Test: {X_test.shape}\")\n",
    "    print(f\"Pos rate => Train: {y_train.mean():.2f}, Val: {y_val.mean():.2f}, Test: {y_test.mean():.2f}\")\n",
    "\n",
    "    # -----------------------------------------------------\n",
    "    # 7) Baseline: Logistic Regression\n",
    "    # -----------------------------------------------------\n",
    "    print(\"\\nFitting LogisticRegression on TRAIN only...\")\n",
    "    lr_clf = LogisticRegression(max_iter=LOGREG_MAX_ITER)\n",
    "    lr_clf.fit(X_train, y_train)\n",
    "    y_prob_lr = lr_clf.predict_proba(X_test)[:, 1]\n",
    "    auc_lr = roc_auc_score(y_test, y_prob_lr)\n",
    "    print(f\"[LogReg] Test AUC: {auc_lr:.4f}\")\n",
    "\n",
    "    # -----------------------------------------------------\n",
    "    # 8) MLP: Hyperparam search\n",
    "    # -----------------------------------------------------\n",
    "    print(\"\\n=== MLP Hyperparam Search ===\")\n",
    "    best_mlp, best_config, best_val_auc = hyperparam_search_mlp(X_train, y_train, X_val, y_val)\n",
    "\n",
    "    # Evaluate best MLP on test\n",
    "    best_mlp.eval()\n",
    "    X_test_t = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        test_out = best_mlp(X_test_t)\n",
    "        test_prob = torch.sigmoid(test_out).cpu().numpy().flatten()\n",
    "\n",
    "    auc_mlp = roc_auc_score(y_test, test_prob)\n",
    "    aupr_mlp = average_precision_score(y_test, test_prob)\n",
    "    preds_mlp = (test_prob > 0.5).astype(int)\n",
    "    f1_mlp = f1_score(y_test, preds_mlp)\n",
    "\n",
    "    print(\"\\n=== Best MLP Model (Test Set) ===\")\n",
    "    print(f\"Config: hidden_dim={best_config[0]}, lr={best_config[1]}, epochs={best_config[2]}\")\n",
    "    print(f\"AUC:   {auc_mlp:.4f}\")\n",
    "    print(f\"AUPR:  {aupr_mlp:.4f}\")\n",
    "    print(f\"F1:    {f1_mlp:.4f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
